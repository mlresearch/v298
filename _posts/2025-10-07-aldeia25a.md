---
title: Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension
  using Large Language Models
abstract: Large language models (LLMs) have demonstrated remarkable capabilities for
  medical question answering and programming, but their potential for generating interpretable
  computable phenotypes (CPs) is under-explored.   In this work, we investigate whether
  LLMs can generate accurate and concise CPs for six clinical phenotypes of varying
  complexity, which could be leveraged to enable scalable clinical decision support
  to improve care for patients with hypertension. In addition to evaluating zero-short
  performance, we propose and test a synthesize, execute, debug, instruct strategy
  that uses LLMs to generate and iteratively refine CPs using data-driven feedback.   Our
  results show that LLMs, coupled with iterative learning, can generate interpretable
  and reasonably accurate programs that approach the performance of state-of-the-art
  ML methods while requiring significantly fewer training examples.
booktitle: Machine Learning for Healthcare 2025
year: '2025'
url: https://openreview.net/forum?id=2QyldF99hc
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: aldeia25a
month: 0
tex_title: Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension
  using Large Language Models
cycles: false
bibtex_author: Aldeia, Guilherme Seidyo Imai and Herman, Daniel S and Cava, William
  La
author:
- given: Guilherme Seidyo Imai
  family: Aldeia
- given: Daniel S
  family: Herman
- given: William La
  family: Cava
date: 2025-10-07
address:
container-title: Proceedings of the 10th Machine Learning for Healthcare Conference
volume: '298'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 10
  - 7
pdf: https://raw.githubusercontent.com/mlresearch/v298/main/assets/aldeia25a/aldeia25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
